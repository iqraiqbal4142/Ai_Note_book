{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**Hyperparameter tuning**, also known as hyperparameter optimization or hyperparameter search, is a crucial step in machine learning model development. Hyperparameters are parameters that are not learned from the data but are set before the training process begins. Tuning these hyperparameters is essential to find the best configuration for your machine learning model, as it can significantly impact the model's performance.\n",
        "\n",
        "# Here's an overview of the hyperparameter tuning process:\n",
        "\n",
        "Selection of Hyperparameters: Before you can tune hyperparameters, you need to identify which hyperparameters are relevant to your model. These could include learning rate, batch size, number of hidden layers in a neural network, the number of trees in a random forest, etc.\n",
        "\n",
        "## Define a Search Space:\n",
        "For each hyperparameter, you need to define a range or set of values that you want to search through during the tuning process. For example, you might specify a learning rate to be searched in the range [0.001, 0.01, 0.1, 1.0].\n",
        "\n",
        "#Choose a Search Strategy:\n",
        " There are several strategies to explore the hyperparameter space, including grid search, random search, and more advanced methods like Bayesian optimization and genetic algorithms. The choice of search strategy depends on the complexity of your problem and the computational resources available.\n",
        "\n",
        "#Evaluation Metric:\n",
        " Define a metric or metrics that you will use to evaluate the performance of the model for each combination of hyperparameters. Common metrics include accuracy, F1 score, mean squared error (MSE), etc. The choice of metric should align with your specific machine learning task (classification, regression, etc.).\n",
        "\n",
        "#Cross-Validation:\n",
        "To avoid overfitting and obtain a robust estimate of model performance, it's essential to use cross-validation during hyperparameter tuning. Cross-validation involves splitting your dataset into multiple subsets (folds) and training/evaluating the model on different combinations of these subsets.\n",
        "\n",
        "#Search and Optimization:\n",
        " Run the hyperparameter search using the chosen strategy and evaluate the model's performance on each set of hyperparameters using cross-validation. The goal is to find the hyperparameters that yield the best performance according to your chosen evaluation metric.\n",
        "\n",
        "#Iterate and Refine:\n",
        "Based on the results of your initial search, you can narrow down the search space and perform additional rounds of tuning to further refine the hyperparameters.\n",
        "\n",
        "#Final Evaluation:\n",
        "Once you've found the best hyperparameters, train the final model using these values on the entire training dataset. Evaluate the model on a separate test dataset to estimate its real-world performance.\n",
        "\n",
        "#Deployment:\n",
        "Deploy the tuned model in your application or use it for your specific machine learning task.\n",
        "\n",
        "Hyperparameter tuning can be a resource-intensive process, as it often involves training and evaluating multiple models. Therefore, it's essential to balance computational resources with the desire to find the best hyperparameters. Automated hyperparameter tuning tools and frameworks like GridSearchCV, RandomizedSearchCV, and libraries like Optuna or Hyperopt can help streamline this process.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "G4jSw96ZHbDt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary libraries\n",
        "import numpy as np\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.model_selection import RandomizedSearchCV, train_test_split\n",
        "from scipy.stats import randint\n",
        "\n",
        "# Load the dataset\n",
        "data = load_iris()\n",
        "X = data.data\n",
        "y = data.target\n",
        "\n",
        "# Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Define the hyperparameter search space\n",
        "param_dist = {\n",
        "    'n_estimators': randint(10, 1000),            # Number of trees in the forest\n",
        "    'max_depth': randint(1, 20),                 # Maximum depth of the trees\n",
        "    'min_samples_split': randint(2, 20),         # Minimum samples required to split an internal node\n",
        "    'min_samples_leaf': randint(1, 20),          # Minimum samples required to be at a leaf node\n",
        "    'bootstrap': [True, False],                  # Whether to bootstrap samples\n",
        "    'criterion': ['gini', 'entropy']            # Split criterion\n",
        "}\n",
        "\n",
        "# Create a Random Forest Classifier\n",
        "rf = RandomForestClassifier()\n",
        "\n",
        "# Create a RandomizedSearchCV object\n",
        "random_search = RandomizedSearchCV(\n",
        "    rf,\n",
        "    param_distributions=param_dist,\n",
        "    n_iter=100,                                   # Number of random combinations to try\n",
        "    scoring='accuracy',                          # Evaluation metric\n",
        "    cv=5,                                        # Number of cross-validation folds\n",
        "    n_jobs=-1,                                   # Use all available CPU cores\n",
        "    random_state=42                              # Random seed for reproducibility\n",
        ")\n",
        "\n",
        "# Perform the random search to find the best hyperparameters\n",
        "random_search.fit(X_train, y_train)\n",
        "\n",
        "# Print the best hyperparameters and corresponding accuracy\n",
        "print(\"Best Hyperparameters:\", random_search.best_params_)\n",
        "print(\"Best Accuracy:\", random_search.best_score_)\n",
        "\n",
        "# Evaluate the model with the best hyperparameters on the test set\n",
        "best_rf = random_search.best_estimator_\n",
        "test_accuracy = best_rf.score(X_test, y_test)\n",
        "print(\"Test Accuracy with Best Hyperparameters:\", test_accuracy)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v1rm2t_6HuWv",
        "outputId": "d316b9e4-7144-4bed-d4c1-77ba1f1888fb"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best Hyperparameters: {'bootstrap': True, 'criterion': 'gini', 'max_depth': 1, 'min_samples_leaf': 5, 'min_samples_split': 11, 'n_estimators': 240}\n",
            "Best Accuracy: 0.9583333333333334\n",
            "Test Accuracy with Best Hyperparameters: 1.0\n"
          ]
        }
      ]
    }
  ]
}