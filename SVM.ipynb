{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMEi5BTzhQ+5h8fKNh9ZFGm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/iqraiqbal4142/Ai_Note_book/blob/main/SVM.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Support Vector Machines (SVMs) are a class of supervised machine learning algorithms used for classification and regression tasks. They are powerful and versatile models that can handle both linear and non-linear relationships between features and targets. SVMs are particularly well-suited for binary classification problems but can be extended to multi-class classification as well.\n",
        "\n",
        "Here are the key concepts and characteristics of Support Vector Machines (SVMs):\n",
        "\n",
        "# Linear Separation:\n",
        "The fundamental idea behind SVMs is to find the hyperplane that best separates the data into two classes in a high-dimensional space. This hyperplane is selected in such a way that it maximizes the margin, which is the distance between the hyperplane and the nearest data points of both classes. These nearest data points are called support vectors.\n",
        "\n",
        "# Kernel Trick:\n",
        " SVMs can handle non-linearly separable data by using a kernel function. A kernel function implicitly maps the data into a higher-dimensional space where a hyperplane can separate it linearly. Common kernel functions include the linear kernel, polynomial kernel, and radial basis function (RBF) kernel.\n",
        "\n",
        "#Margin and Hyperparameters:\n",
        "The margin is a key concept in SVMs. A larger margin indicates a more robust and generalized model. SVMs have hyperparameters like the regularization parameter (C) that control the trade-off between maximizing the margin and minimizing classification errors. The choice of hyperparameters can significantly impact the model's performance.\n",
        "\n",
        "#Soft Margin SVM:\n",
        "In practical situations, it's common for data to have some degree of noise or overlap between classes. Soft Margin SVMs allow for a certain amount of misclassification (violations of the margin) to handle such cases. The parameter C controls the balance between maximizing the margin and allowing misclassifications.\n",
        "\n",
        "#Multi-Class Classification:\n",
        " SVMs are originally designed for binary classification, but they can be extended to multi-class problems using techniques like one-vs-one (OvO) or one-vs-all (OvA) classification.\n",
        "\n",
        "#Regression (SVR):\n",
        "In addition to classification, SVMs can be used for regression tasks. Support Vector Regression (SVR) aims to find a hyperplane that fits the data as closely as possible while allowing for a margin of error.\n",
        "\n",
        "#Outliers and Robustness:\n",
        "SVMs are inherently robust to outliers because they focus on support vectors and maximizing the margin, which reduces the influence of outliers.\n",
        "\n",
        "#Scalability:\n",
        " SVMs can be computationally expensive, especially for large datasets. However, there are techniques such as stochastic gradient descent (SGD) and kernel approximation methods that can make SVMs more scalable.\n",
        "\n",
        "#Popular Libraries:\n",
        " SVMs are widely implemented in various machine learning libraries, with scikit-learn (Python), LIBSVM, and SVMlight being popular choices.\n",
        "\n",
        "SVMs have proven to be effective in a wide range of applications, including image classification, text classification, spam detection, and bioinformatics. When working with SVMs, it's essential to choose an appropriate kernel function and tune the hyperparameters to achieve the best performance on your specific problem.\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "sb5Wm5DljaDE"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fHNMGfpuJ1MX",
        "outputId": "8a90de14-a2fd-42dd-f8d2-5648e7cf476d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Data point 1: Predicted class = 0\n",
            "Data point 2: Predicted class = 1\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "from sklearn import datasets\n",
        "from sklearn import svm\n",
        "\n",
        "# Step 1: Train an SVM model (you can use the Iris dataset or your own data)\n",
        "iris = datasets.load_iris()\n",
        "X_train = iris.data[:, :2]  # Use only the first two features for simplicity\n",
        "y_train = iris.target\n",
        "\n",
        "clf = svm.SVC(kernel='linear')\n",
        "clf.fit(X_train, y_train)\n",
        "\n",
        "# Step 2: Collect new live data (for example, two data points)\n",
        "new_data = np.array([[5.1, 3.5], [6.2, 2.8]])\n",
        "\n",
        "# Step 3: Preprocess the new data (ensure it has the same format as training data)\n",
        "# In this example, we're using the first two features, so make sure new_data has two columns.\n",
        "\n",
        "# Step 4: Use the trained SVM model to make predictions on the new data\n",
        "predictions = clf.predict(new_data)\n",
        "\n",
        "# Display the predictions\n",
        "for i, prediction in enumerate(predictions):\n",
        "    print(f\"Data point {i+1}: Predicted class = {prediction}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "rvVR2rlMKyB_"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}